{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>ZUM</center>\n",
    "# <center> Projekt -- dokumentacja końcowa </center>\n",
    "\n",
    "## <center> Jan Budziński -- 310609 </center>\n",
    "## <center> Jarosław Nachyła -- nr_indeksu </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementacja\n",
    "\n",
    "### 2.1. Nienadzorowana detekcja anomalii\n",
    "\n",
    "W ramach projektu zaimplementowano klasę <i>AnomalyDetector</i>, będącą opakowaniem na algorytmy grupowania KMeans, DBSCAN oraz AgglomerativeClustering, jak również na miary niepodobieństwa euklidesową, Mahalanobisa i Manhattan. Celem tejże klasy jest stworzenie łatwego w obsłudze środowiska testowego, które w miarę możliwości (niektóre algorytmy różnią się wejściami) będzie jednorodne w użyciu dla każdego z porównywanych algorytmów i miar.\n",
    "Klasa ta zawiera następujące metody:\n",
    "\n",
    "- fit -- dopasowuje model do danych wejściowych,\n",
    "- fit_predict -- wykonuje trening modelu i jednocześnie przewidująca klasy i określająca dystanse od centrów klastrów dla danych wejściowych,\n",
    "- transform_distances -- określa, czy dane wejściowe są anomaliami na podstawie dystansu otrzymanego z wybranej miary niepodobieństwa,\n",
    "- transform_labels -- dla liczby klastrów większych niż 2, metoda ta zmienia przypisanie do wszystkich klastrów poza najliczniejszym w anomalię.\n",
    "\n",
    "\n",
    "Dzięki tym metodom użytkownik jest w stanie w łatwy sposób testować różne modele i miary niepodobieństwa, zmieniając wyłącznie jeden parametr w kodzie.\n",
    "\n",
    "Kod implementujący tę klasę jest w pliku anomaly_detector.py.\n",
    "\n",
    "Ponadto utworzono klasę <i>AnomalyDetectorEvaluator</i> zawierającą metody obliczające metryki potrzebne do ewaluacji wytrenowanych modeli.\n",
    "\n",
    "Testowane metryki to:\n",
    "\n",
    "- dokładność (accuracy)\n",
    "- precyzja (precision)\n",
    "- czułość (recall)\n",
    "- pole pod wykresem PRC\n",
    "\n",
    "Wszystkie te metryki zostały dokładniej opisane w dokumentacji wstępnej. Jednakże, z uwagi na fakt, iż dane z założenia są wysoce niezbalansowane (jako że są to dane anomalii, to wejścia o pozytywnej klasie stanowią poniżej 1% wszystkich) uznano, że zwykła dokładność może niewiele powiedzieć, jako że przypisanie wszystkim danym klasy negatywnej pozwala osiągnąć powyżej 99% poprawnych predykcji. W tym celu dodano metrykę dokładności wykrywania outlierów, która testuje, ile spośród prawdziwych outlierów zostało poprawnie zidentyfikowanych. \n",
    "\n",
    "Kod z klasą AnomalyDetectorEvaluator znajduje się w pliku metrics.py.\n",
    "\n",
    "### 2.2. Jednoklasowy klasyfikator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Eksperymenty\n",
    "\n",
    "### 3.1 Porównanie modeli grupowania\n",
    "\n",
    "W ramach pierwszego eksperymentu wykonano testy porównawcze iloczynu kartezjańskiego 3 wybranych modeli, 3 miar niepodobieństwa i 2 zbiorów danych, czyli łącznie 18 pojedynczych testów.\n",
    "\n",
    "Podczas przeprowadzania eksperymentu wystąpiły pewne problemy. Mianowicie, algorytm AgglomerativeClustering cechuje się wysokimi złożonościami: złożoność obliczeniowa to $O(n^2)$, a czasowa to $O(n^3)$. Z tego względu trening na całych zbiorach danych, a w szczególności na zbiorze HTTP był niemożliwy. W związku z tym zastosowano subsampling.\n",
    "\n",
    "Wyniki testów przedstawione są w poniższych tabelach.\n",
    "\n",
    "**_Zbiór Shuttle_**\n",
    "\n",
    "| miara \\ model | **KMeans** | **DBSCAN** | **AgglomerativeClustering** |\n",
    "| ------------- | ---------- | ---------- | --------------------------- |\n",
    "| euklidesowa   |            |            |                             |\n",
    "| Mahalanobisa  |            |            |                             |\n",
    "| Manhattan     |            |            |                             |\n",
    "\n",
    "**_Zbiór HTTP_**\n",
    "\n",
    "| miara \\ model | **KMeans** | **DBSCAN** | **AgglomerativeClustering** |\n",
    "| ------------- | ---------- | ---------- | --------------------------- |\n",
    "| euklidesowa   |            |            |                             |\n",
    "| Mahalanobisa  |            |            |                             |\n",
    "| Manhattan     |            |            |                             |\n",
    "\n",
    "### 3.2. Grupowanie vs klasyfikacja jednoklasowa\n",
    "\n",
    "### 3.3. Grupowanie z MetaCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T13:31:59.870403Z",
     "start_time": "2024-06-08T13:31:58.586279Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from anomaly_detector import AnomalyDetector\n",
    "from metrics import AnomalyDetectorEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T13:32:01.577518Z",
     "start_time": "2024-06-08T13:32:01.430390Z"
    }
   },
   "outputs": [],
   "source": [
    "shuttle = scipy.io.loadmat(\"shuttle.mat\")\n",
    "shuttle_data = pd.DataFrame(shuttle['X'])\n",
    "shuttle_eval = pd.DataFrame(shuttle['y'])\n",
    "http_data = pd.read_csv(\"http_train.csv\")\n",
    "http_eval = pd.read_csv(\"http_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T13:32:03.610961Z",
     "start_time": "2024-06-08T13:32:03.605387Z"
    }
   },
   "outputs": [],
   "source": [
    "shuttle_eval = np.array(shuttle_eval).flatten()\n",
    "http_eval = np.array(http_eval).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T13:11:15.801308Z",
     "start_time": "2024-06-08T13:11:15.799001Z"
    }
   },
   "outputs": [],
   "source": [
    "#shuttle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T21:38:00.043730Z",
     "start_time": "2024-06-15T21:38:00.024161Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'racplusplus'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mracplusplus\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'racplusplus'"
     ]
    }
   ],
   "source": [
    "import racplusplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_data = http_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T21:37:54.706339Z",
     "start_time": "2024-06-15T21:37:54.593608Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'racplusplus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43mracplusplus\u001B[49m\u001B[38;5;241m.\u001B[39mrac(http_data, \u001B[38;5;241m0.95\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m10000\u001B[39m, \u001B[38;5;241m8\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcosine\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Predict clusters\u001B[39;00m\n\u001B[1;32m      5\u001B[0m labels_http_racpp \u001B[38;5;241m=\u001B[39m racpp\u001B[38;5;241m.\u001B[39mlabels_\n",
      "\u001B[0;31mNameError\u001B[0m: name 'racplusplus' is not defined"
     ]
    }
   ],
   "source": [
    "labels = racplusplus.rac(http_data, 0.95, None, 10000, 8, \"cosine\")\n",
    "\n",
    "\n",
    "# Predict clusters\n",
    "labels_http_racpp = racpp.labels_\n",
    "\n",
    "# Compute distances or anomaly scores if available\n",
    "distances_http_racpp = racpp.distances_  # Hypothetical attribute\n",
    "\n",
    "# Transform labels and distances for anomaly detection\n",
    "transformed_labels = AnomalyDetector.transform_labels(labels_http_racpp)\n",
    "transformed_distances = AnomalyDetector.transform_distances(distances_http_racpp)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator_http_racpp = AnomalyDetectorEvaluator(http_eval, transformed_labels, transformed_distances)\n",
    "accuracy_http_racpp = evaluator_http_racpp.calculate_accuracy()\n",
    "recall_http_racpp = evaluator_http_racpp.calculate_recall()\n",
    "precision_http_racpp = evaluator_http_racpp.calculate_precision()\n",
    "auc_pr_http_racpp = evaluator_http_racpp.calculate_auc_pr()\n",
    "\n",
    "# Print the results\n",
    "print(accuracy_http_racpp, recall_http_racpp, precision_http_racpp, auc_pr_http_racpp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = AnomalyDetector(model=\"kmeans\", n_clusters=2)\n",
    "labels_shuttle_kmeans, distances_shuttle_kmeans = kmeans.fit_predict(data=shuttle_data)\n",
    "labels_shuttle_kmeans = AnomalyDetector.transform_labels(labels_shuttle_kmeans)\n",
    "distances_shuttle_kmeans = AnomalyDetector.transform_distances(distances_shuttle_kmeans)\n",
    "evaluator_shuttle_kmeans = AnomalyDetectorEvaluator(shuttle_eval, labels_shuttle_kmeans, distances_shuttle_kmeans)\n",
    "accuracy_shuttle_kmeans = evaluator_shuttle_kmeans.calculate_accuracy()\n",
    "recall_shuttle_kmeans = evaluator_shuttle_kmeans.calculate_recall()\n",
    "precision_shuttle_kmeans = evaluator_shuttle_kmeans.calculate_precision()\n",
    "auc_pr_shuttle_kmeans = evaluator_shuttle_kmeans.calculate_auc_pr()\n",
    "accuracy_shuttle_kmeans, recall_shuttle_kmeans, precision_shuttle_kmeans, auc_pr_shuttle_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T13:32:17.572663Z",
     "start_time": "2024-06-08T13:32:17.559101Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                         Positive Prediction        Negative Prediction\n",
       " 0  Positive Class   True Positive (TP) 4.94%  False Negative (FN) 2.22%\n",
       " 1  Negative Class  False Positive (FP) 0.29%  True Negative (TN) 92.56%,\n",
       "                Metric   Value\n",
       " 0     Positive Recall  69.01%\n",
       " 1     Negative Recall  99.68%\n",
       " 2  Positive Precision  94.39%\n",
       " 3  Negative Precision  97.66%)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_shuttle_kmeans.imbalanced_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-08T13:11:15.960402Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluator_shuttle_kmeans.calculate_outliers_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = AnomalyDetector(model=\"dbscan\", )\n",
    "labels_shuttle_dbscan, distances_shuttle_dbscan = dbscan.fit_predict(data=shuttle_data)\n",
    "labels_shuttle_dbscan = AnomalyDetector.transform_labels(labels_shuttle_dbscan)\n",
    "distances_shuttle_dbscan = AnomalyDetector.transform_distances(distances_shuttle_dbscan)\n",
    "evaluator_shuttle_dbscan = AnomalyDetectorEvaluator(shuttle_eval, labels_shuttle_dbscan, distances_shuttle_dbscan)\n",
    "accuracy_shuttle_dbscan = evaluator_shuttle_dbscan.calculate_accuracy()\n",
    "recall_shuttle_dbscan = evaluator_shuttle_dbscan.calculate_recall()\n",
    "precision_shuttle_dbscan = evaluator_shuttle_dbscan.calculate_precision()\n",
    "auc_pr_shuttle_dbscan = evaluator_shuttle_dbscan.calculate_auc_pr()\n",
    "accuracy_shuttle_dbscan, recall_shuttle_dbscan, precision_shuttle_dbscan, auc_pr_shuttle_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_shuttle_dbscan.calculate_outliers_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 10000\n",
    "\n",
    "all_labels = []\n",
    "all_distances = []\n",
    "\n",
    "for start in range(0, len(shuttle_data), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch_data = shuttle_data[start:end]\n",
    "\n",
    "    agglomerative = AnomalyDetector(model=\"agglomerative\", n_clusters=2)\n",
    "    labels_batch, distances_batch = agglomerative.fit_predict(data=batch_data)\n",
    "\n",
    "    all_labels.extend(AnomalyDetector.transform_labels(labels_batch))\n",
    "    all_distances.extend(AnomalyDetector.transform_distances(distances_batch))\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_distances = np.array(all_distances)\n",
    "\n",
    "evaluator_shuttle_agglomerative = AnomalyDetectorEvaluator(shuttle_eval, all_labels, all_distances)\n",
    "accuracy_shuttle_agglomerative = evaluator_shuttle_agglomerative.calculate_accuracy()\n",
    "recall_shuttle_agglomerative = evaluator_shuttle_agglomerative.calculate_recall()\n",
    "precision_shuttle_agglomerative = evaluator_shuttle_agglomerative.calculate_precision()\n",
    "auc_pr_shuttle_agglomerative = evaluator_shuttle_agglomerative.calculate_auc_pr()\n",
    "\n",
    "accuracy_shuttle_agglomerative, recall_shuttle_agglomerative, precision_shuttle_agglomerative, auc_pr_shuttle_agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_shuttle_agglomerative.calculate_outliers_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative = AnomalyDetector(model=\"agglomerative\", n_clusters=2)\n",
    "labels_shuttle_agglomerative, distances_shuttle_agglomerative = agglomerative.fit_predict(data=shuttle_data)\n",
    "labels_shuttle_agglomerative = AnomalyDetector.transform_labels(labels_shuttle_agglomerative)\n",
    "distances_shuttle_agglomerative = AnomalyDetector.transform_distances(distances_shuttle_agglomerative)\n",
    "evaluator_shuttle_agglomerative = AnomalyDetectorEvaluator(shuttle_eval, labels_shuttle_agglomerative, distances_shuttle_agglomerative)\n",
    "accuracy_shuttle_agglomerative = evaluator_shuttle_agglomerative.calculate_accuracy()\n",
    "recall_shuttle_agglomerative = evaluator_shuttle_agglomerative.calculate_recall()\n",
    "precision_shuttle_agglomerative = evaluator_shuttle_agglomerative.calculate_precision()\n",
    "auc_pr_shuttle_agglomerative = evaluator_shuttle_agglomerative.calculate_auc_pr()\n",
    "accuracy_shuttle_agglomerative, recall_shuttle_agglomerative, precision_shuttle_agglomerative, auc_pr_shuttle_agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-08T13:11:15.976635Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = AnomalyDetector(model=\"kmeans\", n_clusters=2)\n",
    "labels_http_kmeans, distances_http_kmeans = kmeans.fit_predict(data=http_data)\n",
    "labels_http_kmeans = AnomalyDetector.transform_labels(labels_http_kmeans)\n",
    "distances_http_kmeans = AnomalyDetector.transform_distances(distances_http_kmeans)\n",
    "evaluator_http_kmeans = AnomalyDetectorEvaluator(http_eval, labels_http_kmeans, distances_http_kmeans)\n",
    "accuracy_http_kmeans = evaluator_http_kmeans.calculate_accuracy()\n",
    "recall_http_kmeans = evaluator_http_kmeans.calculate_recall()\n",
    "precision_http_kmeans = evaluator_http_kmeans.calculate_precision()\n",
    "auc_pr_http_kmeans = evaluator_http_kmeans.calculate_auc_pr()\n",
    "accuracy_http_kmeans, recall_http_kmeans, precision_http_kmeans, auc_pr_http_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_http_kmeans.calculate_outliers_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-08T13:11:15.978751Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-08T13:11:15.980824Z"
    }
   },
   "outputs": [],
   "source": [
    "dbscan = AnomalyDetector(model=\"dbscan\", )\n",
    "labels_http_dbscan, distances_http_dbscan = dbscan.fit_predict(data=http_data)\n",
    "labels_http_kmeans = AnomalyDetector.transform_labels(labels_http_kmeans)\n",
    "distances_http_kmeans = AnomalyDetector.transform_distances(distances_http_kmeans)\n",
    "evaluator_http_dbscan = AnomalyDetectorEvaluator(http_eval, labels_http_dbscan, distances_http_dbscan)\n",
    "accuracy_http_dbscan = evaluator_http_dbscan.calculate_accuracy()\n",
    "recall_http_dbscan = evaluator_http_dbscan.calculate_recall()\n",
    "precision_http_dbscan = evaluator_http_dbscan.calculate_precision()\n",
    "auc_pr_http_dbscan = evaluator_http_dbscan.calculate_auc_pr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_http_dbscan.calculate_outliers_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-08T13:11:15.983450Z"
    }
   },
   "outputs": [],
   "source": [
    "agglomerative = AnomalyDetector(model=\"agglomerative\", n_clusters=2)\n",
    "labels_http_agglomerative, distances_http_agglomerative = agglomerative.fit_predict(data=http_data)\n",
    "evaluator_http_agglomerative = AnomalyDetectorEvaluator(http_eval, labels_http_agglomerative, distances_http_agglomerative)\n",
    "accuracy_http_agglomerative = evaluator_http_agglomerative.calculate_accuracy()\n",
    "recall_http_agglomerative = evaluator_http_agglomerative.calculate_recall()\n",
    "precision_http_agglomerative = evaluator_http_agglomerative.calculate_precision()\n",
    "auc_pr_http_agglomerative = evaluator_http_agglomerative.calculate_auc_pr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_http_agglomerative.calculate_outliers_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20000\n",
    "\n",
    "\n",
    "for start in range(0, len(http_data), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch_data = http_data[start:end]\n",
    "\n",
    "    agglomerative = AnomalyDetector(model=\"agglomerative\", n_clusters=2)\n",
    "    agglomerative.fit(data=batch_data)\n",
    "\n",
    "all_labels, all_distances = agglomerative.predict(data=http_data)\n",
    "\n",
    "evaluator_http_agglomerative = AnomalyDetectorEvaluator(http_eval, all_labels, all_distances)\n",
    "accuracy_http_agglomerative = evaluator_http_agglomerative.calculate_accuracy()\n",
    "recall_http_agglomerative = evaluator_http_agglomerative.calculate_recall()\n",
    "precision_http_agglomerative = evaluator_http_agglomerative.calculate_precision()\n",
    "auc_pr_http_agglomerative = evaluator_http_agglomerative.calculate_auc_pr()\n",
    "\n",
    "accuracy_http_agglomerative, recall_http_agglomerative, precision_http_agglomerative, auc_pr_http_agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_http_agglomerative.calculate_outliers_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
